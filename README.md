# DEEP NEUTRAL NETWORK

#Visual Storytelling with Cross-Modal Attention Refinement

‚úîÔ∏è Overview

This project focuses on visual storytelling using multimodal learning, where both images and text are used together to predict the next element in a story sequence and is supported by a Notebook template.

The project introduces the idea of Cross-Modal Attention Refinement (CMAR) to improve alignment between visual and textual information, resulting in more coherent and context-aware story predictions.

The model learns to encode k-image caption pairs and generate the (k+1) th caption using:

‚Ä¢	CNN-based image encoder

‚Ä¢	Cross-Modal attention 

‚Ä¢	LSTM / Transformers

‚Ä¢	GRU


‚úîÔ∏è Key purposes:

‚Ä¢	Data loading and preprocessing

‚Ä¢	Model design and explanation

‚Ä¢	Training and evaluation workflows

‚Ä¢	Metric computation and result analysis



‚úîÔ∏è Project Focus

‚Ä¢	Visual storytelling

‚Ä¢	Image‚Äìtext alignment

‚Ä¢	Multimodal reasoning

‚Ä¢	Attention-based models

‚Ä¢	Narrative prediction tasks.


üìà Loss Curves

The training script collects:

‚Ä¢	Train loss per epoch

‚Ä¢	Validation loss per epoch

These are plotted automatically:

‚Ä¢	plt.plot(train_losses)

‚Ä¢	plt.plot(val_losses)

‚úîÔ∏èEvaluation Metrics Used:

‚Ä¢	BLUE

‚Ä¢	CIDE

‚Ä¢	Sequence-level alignment scores


CMAR:

Improving image-text correspondence, making it suitable for sequence modelling tasks without increasing training complexity.

DataSet Used:

Compatible with datasets containing sequences of images + captions (e.g. daniel3303/StoryReasoning).


‚úîÔ∏è Required Installation:

       ‚Ä¢	Python 3.8+ versions

       ‚Ä¢	PyTorch

       ‚Ä¢	matplotlib

       ‚Ä¢	numpy

       ‚Ä¢	torchvision

       ‚Ä¢	Transformers



 ü§ù Contributing

Contributions from the community are highly appreciated.

Proposals including new attention architectures, advanced fusion strategies, or training optimizations are welcome. Please open an issue so we can discuss and plan the improvements together.


  üìú License
  

MIT License.





